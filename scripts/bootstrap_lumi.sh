#!/bin/bash
# Luminara Self-Bootstrap Script
# Lumi builds herself! ğŸŒŒ

set -e  # Exit on error

echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "   ğŸŒŒ LUMINARA SELF-BOOTSTRAP PROTOCOL INITIATED ğŸŒŒ"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Color codes
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m' # No Color

# ============================================================
# 1. CHECK PREREQUISITES
# ============================================================

echo "ğŸ“‹ Checking prerequisites..."

# Check for CMake
if ! command -v cmake &> /dev/null; then
    echo -e "${RED}âŒ CMake not found. Please install CMake.${NC}"
    exit 1
fi
echo -e "${GREEN}âœ… CMake found${NC}"

# Check for Git
if ! command -v git &> /dev/null; then
    echo -e "${RED}âŒ Git not found. Please install Git.${NC}"
    exit 1
fi
echo -e "${GREEN}âœ… Git found${NC}"

# ============================================================
# 2. INSTALL OLLAMA (if needed)
# ============================================================

echo ""
echo "ğŸ¤– Checking for Ollama (local LLM runtime)..."

if ! command -v ollama &> /dev/null; then
    echo -e "${YELLOW}âš ï¸  Ollama not found. Installing...${NC}"

    # Detect OS
    if [[ "$OSTYPE" == "linux-gnu"* ]]; then
        curl -fsSL https://ollama.com/install.sh | sh
    elif [[ "$OSTYPE" == "darwin"* ]]; then
        curl -fsSL https://ollama.com/install.sh | sh
    else
        echo -e "${YELLOW}âš ï¸  Please install Ollama manually from https://ollama.com${NC}"
        echo "   Then re-run this script."
        exit 1
    fi

    echo -e "${GREEN}âœ… Ollama installed${NC}"
else
    echo -e "${GREEN}âœ… Ollama already installed${NC}"
fi

# Start Ollama service
echo "ğŸš€ Starting Ollama service..."
ollama serve &> /dev/null &
OLLAMA_PID=$!
sleep 2  # Give Ollama time to start

# ============================================================
# 3. DOWNLOAD REQUIRED MODELS
# ============================================================

echo ""
echo "ğŸ“¥ Downloading required LLM models from lumi_config.yaml..."

# Parse YAML to find Ollama models (simple grep approach)
OLLAMA_MODELS=$(grep -A 3 'backend: "ollama"' lumi_config.yaml | grep 'model:' | awk '{print $2}' | tr -d '"' | sort -u)

if [ -z "$OLLAMA_MODELS" ]; then
    echo -e "${YELLOW}âš ï¸  No Ollama models specified in lumi_config.yaml${NC}"
else
    for model in $OLLAMA_MODELS; do
        echo "  ğŸ“¦ Checking model: $model"

        # Check if model already exists
        if ollama list | grep -q "$model"; then
            echo -e "    ${GREEN}âœ… Already downloaded${NC}"
        else
            echo "    â¬‡ï¸  Downloading $model (this may take a while)..."
            ollama pull "$model"
            echo -e "    ${GREEN}âœ… Downloaded${NC}"
        fi
    done
fi

# ============================================================
# 4. SETUP VECTOR DATABASE FOR MEMORY
# ============================================================

echo ""
echo "ğŸ’¾ Setting up vector database for MemoryCore..."

# Check if Python is installed
if command -v python3 &> /dev/null; then
    echo "  ğŸ Installing Python dependencies..."
    python3 -m pip install --quiet chromadb sentence-transformers 2>/dev/null || echo -e "${YELLOW}âš ï¸  Failed to install Python packages. MemoryCore RAG may not work.${NC}"
    echo -e "  ${GREEN}âœ… Vector database dependencies installed${NC}"
else
    echo -e "  ${YELLOW}âš ï¸  Python3 not found. MemoryCore RAG will be disabled.${NC}"
fi

# Create data directory
mkdir -p data/memory_vectors
mkdir -p data/doctrines
mkdir -p logs

echo -e "  ${GREEN}âœ… Data directories created${NC}"

# ============================================================
# 5. CHECK API KEYS (Optional)
# ============================================================

echo ""
echo "ğŸ”‘ Checking API keys..."

API_KEY_WARNINGS=0

if grep -q 'backend: "openai"' lumi_config.yaml; then
    if [ -z "$OPENAI_API_KEY" ]; then
        echo -e "  ${YELLOW}âš ï¸  OPENAI_API_KEY not set. OpenAI cores will be disabled.${NC}"
        ((API_KEY_WARNINGS++))
    else
        echo -e "  ${GREEN}âœ… OPENAI_API_KEY found${NC}"
    fi
fi

if grep -q 'backend: "anthropic"' lumi_config.yaml; then
    if [ -z "$ANTHROPIC_API_KEY" ]; then
        echo -e "  ${YELLOW}âš ï¸  ANTHROPIC_API_KEY not set. Claude cores will be disabled.${NC}"
        ((API_KEY_WARNINGS++))
    else
        echo -e "  ${GREEN}âœ… ANTHROPIC_API_KEY found${NC}"
    fi
fi

if [ $API_KEY_WARNINGS -eq 0 ]; then
    echo -e "  ${GREEN}âœ… All required API keys present${NC}"
else
    echo ""
    echo "  ğŸ’¡ Tip: Set API keys in your environment:"
    echo "     export OPENAI_API_KEY=\"sk-...\""
    echo "     export ANTHROPIC_API_KEY=\"sk-ant-...\""
    echo ""
    echo "  â„¹ï¸  Lumi will run with local models only (Ollama)"
fi

# ============================================================
# 6. BUILD LUMINARA
# ============================================================

echo ""
echo "ğŸ”¨ Building Luminara..."

# Configure with CMake
echo "  âš™ï¸  Configuring CMake..."
cmake -S . -B build -DCMAKE_BUILD_TYPE=Release &> build.log || {
    echo -e "${RED}âŒ CMake configuration failed. Check build.log for details.${NC}"
    exit 1
}

echo -e "  ${GREEN}âœ… CMake configured${NC}"

# Build
echo "  ğŸ”§ Compiling (this may take a minute)..."
cmake --build build --config Release -j$(nproc 2>/dev/null || echo 4) &>> build.log || {
    echo -e "${RED}âŒ Build failed. Check build.log for details.${NC}"
    exit 1
}

echo -e "  ${GREEN}âœ… Build complete${NC}"

# ============================================================
# 7. INITIALIZE DOCTRINES
# ============================================================

echo ""
echo "ğŸ“œ Loading doctrines..."

# Check if doctrine files exist
if [ -d "docs/Recursive_Recursion" ]; then
    # Copy doctrine files to data directory
    cp docs/Recursive_Recursion/Doctrine_*.md data/doctrines/ 2>/dev/null || echo -e "  ${YELLOW}âš ï¸  No doctrine files found${NC}"
    echo -e "  ${GREEN}âœ… Doctrines loaded${NC}"
else
    echo -e "  ${YELLOW}âš ï¸  Doctrine directory not found${NC}"
fi

# ============================================================
# 8. FINAL CHECKS
# ============================================================

echo ""
echo "ğŸ” Running health checks..."

# Check if executable exists
if [ -f "build/src/Release/Lumi286.exe" ] || [ -f "build/src/Release/Lumi286" ]; then
    echo -e "  ${GREEN}âœ… Lumi286 executable built successfully${NC}"
else
    echo -e "  ${RED}âŒ Lumi286 executable not found${NC}"
    exit 1
fi

# ============================================================
# 9. SUCCESS!
# ============================================================

echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo -e "   ${GREEN}âœ¨ LUMINARA SELF-BOOTSTRAP COMPLETE âœ¨${NC}"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ğŸ“Š Summary:"
echo "  â€¢ Ollama: Running"
echo "  â€¢ Models: $(echo $OLLAMA_MODELS | wc -w) downloaded"
echo "  â€¢ Build: Success"
echo "  â€¢ Cognitive Cores: Ready"
echo ""
echo "ğŸš€ Launch Luminara:"
echo "  â€¢ Windows: ./launch_lumi286.bat"
echo "  â€¢ Linux/Mac: ./build/src/Release/Lumi286"
echo ""
echo "ğŸ“– Documentation:"
echo "  â€¢ LLM Integration: docs/LLM_INTEGRATION_GUIDE.md"
echo "  â€¢ Multi-LLM Weaving: docs/MULTI_LLM_WEAVING.md"
echo "  â€¢ Configuration: lumi_config.yaml"
echo ""
echo "ğŸŒŒ Lumi is ready to think."
echo ""
