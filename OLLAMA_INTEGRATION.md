# Ollama Integration - FREE Local LLM for Luminara

## Quick Start (Automated)

### Windows
```powershell
# Install Ollama and pull model
.\scripts\install_ollama.ps1

# Run physics-guided LLM pipeline
.\build\src\Debug\ollama_pipeline_example.exe
```

### Mac
```bash
# Install Ollama
brew install ollama

# Start Ollama server
ollama serve &

# Pull model
ollama pull llama3.2

# Run pipeline
./build/src/Debug/ollama_pipeline_example
```

### Linux
```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Start server (usually auto-starts)
ollama serve &

# Pull model
ollama pull llama3.2

# Run pipeline
./build/src/Debug/ollama_pipeline_example
```

## What This Does

1. **Physics Simulation**: Your input flows through 8 cognitive attractors
2. **Collapse State**: System settles into stable pattern (takes ~5 seconds)
3. **LLM Bias**: Physics state shapes LLM probability distribution
4. **Response Generation**: Real local LLM (Llama 3.2) generates response

## Architecture

```
User Input: "I'm curious about consciousness..."
     ↓
Codon with Emotional Field (valence=0.4, arousal=0.6)
     ↓
Physics Simulation (8 attractors, 4-5 second collapse)
     ↓
Collapse State:
  - WisdomCore: 40%
  - LogicCore: 30%
  - IntuitionCore: 20%
  - Temperature: 0.88 (from arousal)
     ↓
Ollama (llama3.2) with physics-biased system prompt
     ↓
Response: Physics-guided, not randomly sampled
```

## Free & Local

- **100% Free**: No API costs, ever
- **100% Local**: Runs on your machine, full privacy
- **100% Physics-Based**: Responses guided by attractor collapse

## Available Models

Fast (3-4GB RAM):
- `llama3.2` (3B params) - Recommended
- `phi3` (3.8B params)

Powerful (8-16GB RAM):
- `llama3.1` (8B params)
- `mistral` (7B params)

Change model:
```powershell
ollama pull mistral
# Then edit ollama_pipeline_example.cpp line 120
```

## Performance

| Model | RAM | Speed (tokens/sec) | Quality |
|-------|-----|-------------------|---------|
| llama3.2 | 4GB | 15-30 | Good |
| llama3.1 | 8GB | 10-20 | Better |
| mistral | 8GB | 12-25 | Better |

*Speed depends on CPU/GPU. Apple M1/M2 is very fast.*

## How Physics Guides the LLM

The physics collapse state biases the LLM in multiple ways:

1. **System Prompt**: Core activations embedded in prompt
   ```
   "Active Cores:
   - WisdomCore: 40%
   - LogicCore: 30%"
   ```

2. **Temperature**: Based on emotional arousal
   ```cpp
   temperature = 0.7 + (arousal * 0.3)
   // High arousal (0.8) → temperature 0.94 (more creative)
   // Low arousal (0.2) → temperature 0.76 (more focused)
   ```

3. **Emotional Context**: Valence/arousal embedded
   ```
   "Emotional Field:
   - Valence: -0.6 (negative)
   - Arousal: 0.8 (intense)"
   ```

4. **Primed Symbols**: Active cognitive symbols
   ```
   "Primed Symbols: EMOT_ANXIETY, EMOT_CURIOSITY"
   ```

## Real vs Mock LLM

**MockLLMBackend** (for testing):
- Instant response
- Simulates core-based patterns
- No actual inference

**OllamaBackend** (real LLM):
- 5-30 second response time
- Actual language model inference
- Physics-guided token sampling

## Troubleshooting

**"Ollama not running"**:
```bash
ollama serve
```

**Slow responses**:
- Use smaller model: `ollama pull llama3.2`
- Check RAM usage
- Close other apps

**Connection errors**:
```bash
# Check Ollama status
curl http://localhost:11434/api/tags

# Restart Ollama
killall ollama
ollama serve
```

## Next Steps

1. **Multi-turn conversations**: Track dialogue history
2. **Real-time GUI**: Visualize physics + LLM together
3. **Multiple LLMs**: Different models for different cores
4. **RAG Integration**: Vector DB for long-term memory

## Example Output

```
[8] Generating response with Ollama...
    (This may take 5-30 seconds)

================================================================
  LUMINARA'S RESPONSE (Physics-Guided via Ollama)
================================================================

The emergence of consciousness from physical systems is one of
the most fascinating questions in science and philosophy. From a
physics perspective, consciousness might arise when information
processing reaches a certain threshold of integration and
self-reference...

[Response continues with physics-guided reasoning]
================================================================

This response was generated by a REAL local LLM (Ollama)
guided by physics simulation of cognitive attractors.
100% FREE. 100% LOCAL. 100% PHYSICS-BASED.
```

---

**Built with**:
- Ollama (https://ollama.com)
- Llama 3.2 (Meta)
- PhysX-style attractor physics
- Emotional field dynamics
